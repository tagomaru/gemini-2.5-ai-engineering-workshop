{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dc71dad6",
      "metadata": {
        "id": "dc71dad6"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/philschmid/gemini-2.5-ai-engineering-workshop/blob/main/notebooks/01-text-generation-and-chat.ipynb)\n",
        "\n",
        "# Part 1 - Text Generation and Chat\n",
        "\n",
        "This part focuses on text generation with the Gemini API using the `google-genai` SDK, including basic prompts, chat interactions, streaming, and configuration.\n",
        "\n",
        "Make sure you have completed the [setup and authentication](solution_00_setup_and_authentication.md) section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "17fe7909",
      "metadata": {
        "id": "17fe7909"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "import os\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import userdata\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "else:\n",
        "    GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY',None)\n",
        "\n",
        "# Create client with api key\n",
        "MODEL_ID = \"gemini-2.5-flash-preview-05-20\"\n",
        "client = genai.Client(api_key=GEMINI_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e25baada",
      "metadata": {
        "id": "e25baada"
      },
      "source": [
        "## 1. Send Your First Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d19f5446",
      "metadata": {
        "id": "d19f5446",
        "outputId": "b062350e-3dd3-4e9d-92a7-ad8f5e583f07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Gemini:\n",
            "Here are 3 names for a new coffee shop emphasizing sustainability:\n",
            "\n",
            "1.  **Root & Bloom Coffee:**\n",
            "    *   **Emphasis:** \"Root\" signifies deep connection to the earth, ethical sourcing, and strong foundations. \"Bloom\" signifies growth, flourishing, and the positive impact of sustainable practices. It evokes a full, healthy lifecycle.\n",
            "\n",
            "2.  **The Conscious Cup:**\n",
            "    *   **Emphasis:** \"Conscious\" directly addresses the mindful choices behind sustainability â€“ ethical sourcing, fair trade, environmental impact. \"Cup\" keeps it simple and direct to the product. It implies intentionality and awareness.\n",
            "\n",
            "3.  **Evergreen Grounds:**\n",
            "    *   **Emphasis:** \"Evergreen\" symbolizes continuous life, renewal, and natural beauty, evoking forests and enduring vitality. \"Grounds\" works on two levels: coffee grounds (what you drink) and a place/community. It suggests a lasting, vibrant, and environmentally friendly space.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Create 3 names for a new coffee shop that emphasizes sustainability.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "print(\"Response from Gemini:\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1271d094",
      "metadata": {
        "id": "1271d094"
      },
      "source": [
        "#### **!! Exercise: Sending Various Prompts !!**\n",
        "\n",
        "Practice sending different types of prompts to the Gemini model and observe its responses. You can also experiment with different model versions if they are available to you.\n",
        "\n",
        "Tasks:\n",
        "- Write a prompt to ask Gemini to generate a short poem about a robot.\n",
        "- Write a prompt to ask Gemini to explain \"machine learning\" in simple terms.\n",
        "- Try other models (e.g., `gemini-2.0-flash`) and send your prompts to them and compare the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6329dfcb",
      "metadata": {
        "id": "6329dfcb",
        "outputId": "f60e600e-0029-47c2-c358-d6e57fddfb98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response from Gemini:\n",
            "Metal shell, a blinking light,\n",
            "It hums its purpose, day and night.\n",
            "Of circuits keen and wires spun,\n",
            "A task completed, work well done.\n"
          ]
        }
      ],
      "source": [
        "# TODO:\n",
        "prompt = \"Generate a short poem about a robot.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "print(\"Response from Gemini:\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "148c83ca",
      "metadata": {
        "id": "148c83ca"
      },
      "source": [
        "## 2. Understanding and Counting Tokens\n",
        "\n",
        "Tokens are the basic units that Gemini models use to process text. Understanding token usage is crucial for:\n",
        "- **Cost management**: Billing is based on token consumption\n",
        "- **Context limits**: Models have maximum token limits (e.g., 1M tokens for Gemini 2.5 Pro)\n",
        "- **Performance optimization**: Smaller inputs generally process faster\n",
        "\n",
        "For Gemini models, a token is equivalent to about 4 characters, and 100 tokens equals about 60-80 English words.\n",
        "\n",
        "### Count tokens before generation\n",
        "\n",
        "You can count tokens in your input before sending it to the model to estimate costs and ensure you stay within limits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5ac574ed",
      "metadata": {
        "id": "5ac574ed",
        "outputId": "219a9805-bb17-46a2-e762-c7b4bc77c797",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tokens: 11\n",
            "Estimated input cost: $0.000002\n"
          ]
        }
      ],
      "source": [
        "prompt = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Count tokens in the input\n",
        "# TODO: Call the client.models.count_tokens() method.\n",
        "# Make sure to pass the MODEL_ID and the prompt.\n",
        "token_count = client.models.count_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt\n",
        ")\n",
        "print(f\"Input tokens: {token_count.total_tokens}\")\n",
        "\n",
        "# Estimate cost (example pricing - check current rates)\n",
        "estimated_cost = token_count.total_tokens * 0.15 / 1_000_000\n",
        "print(f\"Estimated input cost: ${estimated_cost:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0b222b5",
      "metadata": {
        "id": "b0b222b5"
      },
      "source": [
        "### Count tokens after generation\n",
        "\n",
        "After generating content, you can access detailed token usage information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "644befed",
      "metadata": {
        "id": "644befed"
      },
      "outputs": [],
      "source": [
        "prompt = \"Write a haiku about artificial intelligence.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "print(f\"Generated haiku:\\n{response.text}\\n\")\n",
        "\n",
        "# Access token usage metadata\n",
        "usage = response.usage_metadata\n",
        "print(f\"Input tokens: {usage.prompt_token_count}\")\n",
        "print(f\"Thought tokens: {usage.thoughts_token_count}\")\n",
        "print(f\"Output tokens: {usage.candidates_token_count}\")\n",
        "\n",
        "# Calculate total estimated cost\n",
        "total_cost = (usage.prompt_token_count * 0.15 + (usage.candidates_token_count + usage.thoughts_token_count) * 3.5) / 1_000_000\n",
        "print(f\"Total estimated cost: ${total_cost:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bebb35a9",
      "metadata": {
        "id": "bebb35a9"
      },
      "source": [
        "## 3. Text Understanding with `contents`\n",
        "\n",
        "The simplest way to generate text is to provide the model with a text-only prompt. `contents` can be a single prompt, a list of prompts, or a combination of multimodal inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72c5ef98",
      "metadata": {
        "id": "72c5ef98"
      },
      "outputs": [],
      "source": [
        "response_capital = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What is the capital of France?\"\n",
        ")\n",
        "print(f\"Q: What is the capital of France?\\nA: {response_capital.text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e19e0c0e",
      "metadata": {
        "id": "e19e0c0e"
      },
      "outputs": [],
      "source": [
        "# TODO: Call the client.models.generate_content() method.\n",
        "# For the contents, provide a list of strings:\n",
        "# 1. \"Create 3 names for a vegan restaurant\"\n",
        "# 2. \"city: Berlin\"\n",
        "# response_restaurant_berlin = client.models.generate_content(\n",
        "#     model=MODEL_ID,\n",
        "#     contents=[...]\n",
        "# )\n",
        "print(f\"\\nVegan restaurant names in Berlin:\\n{response_restaurant_berlin.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "184d9989",
      "metadata": {
        "id": "184d9989"
      },
      "source": [
        "## 4. Streaming Responses\n",
        "\n",
        "Streaming allows you to receive responses incrementally as they're generated, providing a better user experience for long responses or real-time applications like chatbots.\n",
        "\n",
        "**When to use streaming:**\n",
        "- Interactive applications (chatbots, assistants)\n",
        "- Long content generation\n",
        "- Real-time user feedback\n",
        "- Improved perceived performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bfe73e5",
      "metadata": {
        "id": "4bfe73e5"
      },
      "outputs": [],
      "source": [
        "prompt_long_story = \"Write a short story about a brave knight and a friendly dragon.\"\n",
        "\n",
        "print(\"Streaming response:\")\n",
        "for chunk in client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt_long_story\n",
        "):\n",
        "    if chunk.text:  # Check if chunk has text content\n",
        "        print(chunk.text, end=\"\", flush=True)\n",
        "print(\"\\n\")  # Add newline at the end"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f9754bc",
      "metadata": {
        "id": "8f9754bc"
      },
      "source": [
        "## 5. Chat (Multi-turn Conversations)\n",
        "\n",
        "The SDK chat class provides an interface to keep track of conversation history. Behind the scenes it uses the same `generate_content` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "845e6896",
      "metadata": {
        "id": "845e6896"
      },
      "outputs": [],
      "source": [
        "chat_session = client.chats.create(model=MODEL_ID)\n",
        "\n",
        "user_message1 = \"I'm planning a weekend trip. Any suggestions for a city break in Europe?\"\n",
        "print(f\"User: {user_message1}\")\n",
        "response1 = chat_session.send_message(message=user_message1)\n",
        "print(f\"Model: {response1.text}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7f88e29",
      "metadata": {
        "id": "b7f88e29"
      },
      "outputs": [],
      "source": [
        "user_message2 = \"I like history and good food. Not too expensive.\"\n",
        "print(f\"User: {user_message2}\")\n",
        "# TODO: Call the chat_session.send_message() method with user_message2.\n",
        "# response2 = chat_session.send_message(message=...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e3a5c84",
      "metadata": {
        "id": "0e3a5c84"
      },
      "outputs": [],
      "source": [
        "# View conversation history\n",
        "history = chat_session.get_history()\n",
        "print(f\"Total messages in conversation: {len(history)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e0be978",
      "metadata": {
        "id": "8e0be978"
      },
      "source": [
        "## 6. System Instructions\n",
        "\n",
        "System instructions let you define the model's behavior and personality. They're applied consistently throughout the conversation.\n",
        "\n",
        "**Best practices for system instructions:**\n",
        "- Be specific and clear\n",
        "- Define the role and tone\n",
        "- Include formatting preferences\n",
        "- Set behavioral guidelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23844462",
      "metadata": {
        "id": "23844462"
      },
      "outputs": [],
      "source": [
        "system_instruction_poet = \"You are a renowned poet from the 17th century, specializing in sonnets. Respond in iambic pentameter and use eloquent, period-appropriate language.\"\n",
        "\n",
        "response_poet = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What are your thoughts on modern technology?\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        system_instruction=system_instruction_poet\n",
        "    )\n",
        ")\n",
        "print(f\"\\nPoet model on modern tech:\\n{response_poet.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09954e6a",
      "metadata": {
        "id": "09954e6a"
      },
      "source": [
        "## 7. Generation Configuration\n",
        "\n",
        "Customize the generation behavior using configuration parameters. Understanding these helps you fine-tune responses for your specific use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "289d6553",
      "metadata": {
        "id": "289d6553"
      },
      "outputs": [],
      "source": [
        "# Configuration using dictionary\n",
        "generation_config_dict = {\n",
        "    \"temperature\": 0.2,      # Lower = more deterministic, higher = more creative\n",
        "    \"max_output_tokens\": 2000, # Limit response length\n",
        "    \"top_p\": 0.8,            # Nucleus sampling - diversity of token selection\n",
        "    \"top_k\": 30,             # Consider top 30 most likely tokens\n",
        "\n",
        "}\n",
        "\n",
        "# TODO: Call client.models.generate_content()\n",
        "# Pass the MODEL_ID, a prompt to \"Write a very short tagline for a new brand of eco-friendly sneakers.\",\n",
        "# and the generation_config_dict.\n",
        "# response_config = client.models.generate_content(\n",
        "#     model=...,\n",
        "#     contents=...,\n",
        "#     config=...\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59d91651",
      "metadata": {
        "id": "59d91651"
      },
      "source": [
        "**Parameter Guide:**\n",
        "- **Temperature (0.0-2.0)**: Controls randomness. Use 0.2-0.4 for factual content, 0.7-1.0 for creative content\n",
        "- **Top-p (0.0-1.0)**: Controls diversity. Lower values = more focused, higher = more diverse\n",
        "- **Top-k**: Limits token choices. Lower = more focused, higher = more diverse\n",
        "- **Max output tokens**: Prevents overly long responses and controls costs\n",
        "\n",
        "## 8. Long Context and File Uploads\n",
        "\n",
        "Gemini 2.5 Pro has a 1M token context window. In practice, 1 million tokens could look like:\n",
        "\n",
        "- 50,000 lines of code (with the standard 80 characters per line)\n",
        "- All the text messages you have sent in the last 5 years\n",
        "- 8 average length English novels\n",
        "- 1 hour of video data\n",
        "\n",
        "The File API allows you to upload files to the Gemini API and use them as context for your requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0964dc9",
      "metadata": {
        "id": "c0964dc9"
      },
      "outputs": [],
      "source": [
        "# Example with a text file (more reliable than the audio example)\n",
        "import requests\n",
        "\n",
        "# Download a sample text file\n",
        "sample_text_url = \"https://www.gutenberg.org/files/74/74-0.txt\"  # Adventures of Tom Sawyer\n",
        "response_req = requests.get(sample_text_url)\n",
        "\n",
        "# Save to local file\n",
        "with open(\"sample_book.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(response_req.text)\n",
        "\n",
        "# Upload the file to the Gemini API\n",
        "try:\n",
        "    myfile = client.files.upload(file=\"sample_book.txt\")\n",
        "    print(f\"File uploaded successfully: {myfile.name}\")\n",
        "\n",
        "    # Generate content using the uploaded file as context\n",
        "    response = client.models.generate_content(\n",
        "        model=MODEL_ID,\n",
        "        contents=[myfile, \"Summarize this book in 3 key points\"]\n",
        "    )\n",
        "\n",
        "    print(\"Summary:\")\n",
        "    print(response.text)\n",
        "\n",
        "    # Check token usage for the large context\n",
        "    print(f\"\\nToken usage: {response.usage_metadata.total_token_count}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error uploading file: {e}\")\n",
        "    print(\"Make sure the file exists and is accessible\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c465f36e",
      "metadata": {
        "id": "c465f36e"
      },
      "source": [
        "## 9. !! Exercise: Chat with a \"Book\" !!\n",
        "\n",
        "Create an interactive chat session where you can \"talk\" to the book \"Alice in Wonderland\". You'll set up the chat with a specific persona for the AI and use the book's text as context for the conversation.\n",
        "\n",
        "Task:\n",
        "- Download the text of \"Alice in Wonderland\" (a helper code block is provided).\n",
        "- Upload the book's text file (`alice_in_wonderland.txt`) to the Gemini API using `client.files.upload()`.\n",
        "- Create a chat session using `client.chats.create()`:\n",
        "- Send an initial message to the chat session using `chat.send_message()`:\n",
        "- Send at least one follow-up question to the chat session (e.g., \"Explain the various methods of speech delivery in more detail\") and print its response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d22740e2",
      "metadata": {
        "id": "d22740e2"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Download Alice in Wonderland\n",
        "book_text_url = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
        "try:\n",
        "    response_book_req = requests.get(book_text_url)\n",
        "    response_book_req.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "    with open(\"alice_in_wonderland.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(response_book_req.text)\n",
        "    print(\"Book downloaded successfully!\")\n",
        "\n",
        "except requests.RequestException as e:\n",
        "    print(f\"Error downloading book: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34b5b3bc",
      "metadata": {
        "id": "34b5b3bc"
      },
      "outputs": [],
      "source": [
        "# TODO:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c152d709",
      "metadata": {
        "id": "c152d709"
      },
      "outputs": [],
      "source": [
        "# TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76159ebc",
      "metadata": {
        "id": "76159ebc"
      },
      "source": [
        "## Recap & Next Steps\n",
        "\n",
        "**What You've Learned:**\n",
        "- Basic text generation with `client.models.generate_content()` for single prompts\n",
        "- Token counting and cost estimation for better resource management\n",
        "- Streaming responses with `generate_content_stream()` for improved user experience\n",
        "- Multi-turn conversations using `client.chats.create()` and chat sessions\n",
        "- System instructions for consistent model behavior and personality\n",
        "- Generation configuration parameters for fine-tuning responses\n",
        "- Long context handling and file uploads with the File API\n",
        "- Error handling and best practices for production applications\n",
        "\n",
        "**Key Takeaways:**\n",
        "- Monitor token usage to control costs and stay within limits\n",
        "- Use streaming for interactive applications and long responses\n",
        "- Configure parameters based on your use case (factual vs creative content)\n",
        "- Implement proper error handling for robust applications\n",
        "- System instructions are powerful for setting behavior and tone\n",
        "\n",
        "**Next Steps:** Continue with [Part 2: Multimodal Capabilities](https://github.com/philschmid/gemini-2.5-ai-engineering-workshop/blob/main/notebooks/02-multimodal-capabilities.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/philschmid/gemini-2.5-ai-engineering-workshop/blob/main/notebooks/02-multimodal-capabilities.ipynb)\n",
        "\n",
        "**More Resources:**\n",
        "- [Text Generation Guide](https://ai.google.dev/gemini-api/docs/text-generation)\n",
        "- [Token Counting Guide](https://ai.google.dev/gemini-api/docs/tokens)\n",
        "- [Long Context Documentation](https://ai.google.dev/gemini-api/docs/long-context)\n",
        "- [File API Documentation](https://ai.google.dev/gemini-api/docs/files)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}